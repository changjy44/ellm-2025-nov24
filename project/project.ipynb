{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d650ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from langchain_community.document_loaders import UnstructuredEPubLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, JSONLoader, UnstructuredXMLLoader\n",
    "\n",
    "\n",
    "import chromadb\n",
    "from uuid import uuid4\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b2a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def print_chunk_info(chunks):\n",
    "   print(f'No of chunks: {len(chunks)}')\n",
    "   idx = random.randrange(0, len(chunks))\n",
    "   print(f'Chunk index: {idx}')\n",
    "   print('Chunk details')\n",
    "   for k, v in enumerate(chunks[idx]):\n",
    "      print(f'\\t{k} = {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the questions\n",
    "\n",
    "\n",
    "# Import the answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a text splitter\n",
    "chunk_size = 300\n",
    "chunk_overlap = 30\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d64798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Notes (PDF)\n",
    "\n",
    "pdf_loader = PyPDFLoader(file_path='./docs/Path-to-GitOps-Red-Hat-Developer-e-book.pdf', mode=\"page\", extract_images=True)\n",
    "chunks = pdf_loader.load_and_split(text_splitter)\n",
    "\n",
    "print_chunk_info(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "\n",
    "# TODO: Create embedding model\n",
    "embed_model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "#embed_model_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "chroma_embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=embed_model_name)\n",
    "\n",
    "# TODO: Prepare the chunks for inserting into Chroma\n",
    "# Extract the text\n",
    "texts = [ c.page_content for c in chunks ]\n",
    "print(texts[100])\n",
    "print(len(texts))\n",
    "\n",
    "text_ids = [  str(uuid4())[:8] for _ in range(len(texts))]\n",
    "print(text_ids)\n",
    "print(len(text_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fbfaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the chunks embdedings into ChromaDB\n",
    "\n",
    "# TODO: Create ephemeral Chroma client and save chunks\n",
    "col_name = 'carol'\n",
    "\n",
    "# Create a the chromadb client\n",
    "ch_client = chromadb.Client()\n",
    "\n",
    "# drop the table\n",
    "try:\n",
    "   ch_client.delete_collection(col_name)\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Insert the texts into the database\n",
    "carol_col = ch_client.create_collection(\n",
    "   name = col_name,\n",
    "   embedding_function=chroma_embed_func\n",
    ")\n",
    "\n",
    "#Insert the docs into the collection\n",
    "carol_col.add(\n",
    "   documents = texts,\n",
    "   ids = text_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472abfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG retrieval (Individual question)\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Extract the core ideas of the question \n",
    "question = \"What is the name of Scrooge's underpaid clerk?\"\n",
    "#question = \"Who was Scrooge's deceased business partner?\"\n",
    "#question = \"Who was Scrooge engaged to in his youth, and why did she leave him?\"\n",
    "question = \"What is the name of Bob Cratchit's youngest son who is ill?\"\n",
    "#question = \"What does Scrooge see written on the gravestone that frightens him into changing his ways?\"\n",
    "#question = \" What is Scrooge's response when his nephew Fred invites him to Christmas dinner at the beginning of the story?\"\n",
    "question = \" What specific, generous act does Scrooge perform for the Cratchit family on Christmas morning?\"\n",
    "question = \"Who is Voldermort?\"\n",
    "\n",
    "prompt = f\"{question}\\n\\nWhat is sentence that verbalizes this data?\"\n",
    "#prompt = f\"{question}\\n\\nWhat data can be extracted from this sentence?\"\n",
    "#prompt = f\"Generate an approximately fifteen-word sentence that describes all this data: {question}\"\n",
    "\n",
    "# convert to a statement\n",
    "enc_prompt = tokenizer(prompt, return_tensors='pt')\n",
    "enc_answer = model.generate(enc_prompt.input_ids)\n",
    "answer = tokenizer.decode(enc_answer[0], skip_special_tokens=True)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b88bd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Your code\n",
    "context = \"\"\n",
    "results = carol_col.query(\n",
    "   query_texts=[ answer ],\n",
    "   n_results=3\n",
    ")\n",
    "print(results['distances'])\n",
    "for id in results['ids'][0]:\n",
    "   result = carol_col.get(id)\n",
    "   context += result['documents'][0]\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7585ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompt = f\"Answer based on context:\\n\\n{context}\\n\\n{question}\"\n",
    "print(question_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Your code\n",
    "enc_query_prompt = tokenizer(question_prompt, return_tensors='pt')\n",
    "\n",
    "enc_query_answer = model.generate(enc_query_prompt.input_ids)\n",
    "\n",
    "query_answer = tokenizer.decode(enc_query_answer[0], skip_special_tokens=True)\n",
    "\n",
    "print(question)\n",
    "print(query_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer all 31 questions and calculate metrics (Base Model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc2593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer all 31 questions and calculate metrics (RAG Model)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ellm-2025-nov24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
